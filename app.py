# app.py
import os
import re
import time
import torch
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(SCRIPT_DIR, "tinybert_model")

if not os.path.exists(MODEL_PATH):
    raise RuntimeError("‚ùå –ü–∞–ø–∫–∞ 'tinybert_model' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –ü–æ–ª–æ–∂–∏—Ç–µ –µ—ë —Ä—è–¥–æ–º —Å app.py.")

# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
model.to(DEVICE)
model.eval()
print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!")


# === –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===
def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = text.replace('—ë', '–µ')
    replacements = {
        '0': '–æ', '1': '–∏', '3': '–∑', '4': '—á', '7': '—Ç',
        '@': '–∞', '$': '—Å', '+': '—Ç', '*': '', '#': '', '%': '',
        '!': '', '?': '', ',': '', ';': '', ':': '', '"': '', "'": '',
        '(': '', ')': '', '[': '', ']': '', '-': '', '_': '',
        'a': '–∞', 'b': '—å', 'c': '—Å', 'd': "—å", 'e': "–µ", 'f': "—Ñ", 'g': "–¥",
        'h': "—Ö", 'i': "–∏", 'j': "–∏", 'k': "–∫", 'l': "–ª", 'm': "–º", 'n': "–ø", 'o': "–æ", 'p': "—Ä",
        'q': "–¥", 'r': "–≥", 's': "—Å", 't': "—Ç", 'u': "–∏", 'v': "–≤", 'w': "—à", 'x': "—Ö", 'y': "—É", 'z': "–∑"
    }
    for char, repl in replacements.items():
        text = text.replace(char, repl)
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    text = re.sub(r'[^–∞-—è\s]', ' ', text)
    return ' '.join(text.split())

    for k, v in replacements.items():
        text = text.replace(k, v)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    text = re.sub(r'[^–∞-—è\s]', ' ', text)
    return ' '.join(text.split())


def predict_mat(text):
    processed = preprocess_text(text)
    inputs = tokenizer(
        processed,
        truncation=True,
        padding="max_length",
        max_length=96,
        return_tensors="pt"
    )
    with torch.no_grad():
        outputs = model(
            input_ids=inputs["input_ids"].to(DEVICE),
            attention_mask=inputs["attention_mask"].to(DEVICE)
        )
        label = torch.argmax(outputs.logits, dim=1).cpu().item()
    return label == 1  # True –µ—Å–ª–∏ –º–∞—Ç


# === Flask-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ===
app = Flask(__name__, static_folder='.', static_url_path='')


@app.route('/')
def index():
    return app.send_static_file('index.html')


@app.route('/moderate', methods=['POST'])
def moderate():
    data = request.get_json()
    text = data.get('text', '')

    if not text.strip():
        return jsonify({"error": "–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π"}), 400

    try:
        preprocessed_text = preprocess_text(text)
        is_mat = predict_mat(preprocessed_text)
        return jsonify({"result": 1 if is_mat else 0})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    print("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω: http://localhost:5001")
    app.run(host='0.0.0.0', port=5002, debug=False)